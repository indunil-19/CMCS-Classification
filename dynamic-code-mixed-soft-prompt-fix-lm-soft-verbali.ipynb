{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/indunil19/dynamic-code-mixed-soft-prompt-fix-lm-soft-verbali?scriptVersionId=115963343\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"pip install openprompt","metadata":{"id":"GJnzG561XzuD","outputId":"8a1c54ef-d5f9-4ac9-9b92-6dff736b9344","execution":{"iopub.status.busy":"2023-01-07T07:08:34.516915Z","iopub.execute_input":"2023-01-07T07:08:34.517677Z","iopub.status.idle":"2023-01-07T07:09:17.369073Z","shell.execute_reply.started":"2023-01-07T07:08:34.517575Z","shell.execute_reply":"2023-01-07T07:09:17.367738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2023-01-07T07:09:17.371769Z","iopub.execute_input":"2023-01-07T07:09:17.372296Z","iopub.status.idle":"2023-01-07T07:09:28.012735Z","shell.execute_reply.started":"2023-01-07T07:09:17.372255Z","shell.execute_reply":"2023-01-07T07:09:28.011373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom transformers import TrainingArguments, Trainer\nimport evaluate\nfrom datasets import load_metric\n","metadata":{"id":"6BIRiGiWkVU6","execution":{"iopub.status.busy":"2023-01-07T07:09:28.014823Z","iopub.execute_input":"2023-01-07T07:09:28.015248Z","iopub.status.idle":"2023-01-07T07:09:43.135794Z","shell.execute_reply.started":"2023-01-07T07:09:28.015202Z","shell.execute_reply":"2023-01-07T07:09:43.134697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_type = \"XLM-R\" #@param [\"SinBERT\", \"Bert\", \"XLM-R\"]\ntechnique = \"sentiment\" #@param [\"humor\", \"hate speech\", \"sentiment\"]\nrandom_state = 42 #@param","metadata":{"id":"JUCDlx1akOCt","execution":{"iopub.status.busy":"2023-01-07T07:09:43.138548Z","iopub.execute_input":"2023-01-07T07:09:43.13942Z","iopub.status.idle":"2023-01-07T07:09:43.145597Z","shell.execute_reply.started":"2023-01-07T07:09:43.139377Z","shell.execute_reply":"2023-01-07T07:09:43.144518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from openprompt.data_utils.text_classification_dataset import PROCESSORS\nfrom datasets import load_dataset","metadata":{"id":"jF3gImOqX0V8","execution":{"iopub.status.busy":"2023-01-07T07:09:43.146967Z","iopub.execute_input":"2023-01-07T07:09:43.147358Z","iopub.status.idle":"2023-01-07T07:09:43.324368Z","shell.execute_reply.started":"2023-01-07T07:09:43.147319Z","shell.execute_reply":"2023-01-07T07:09:43.32344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_path=\"/kaggle/input/sentence-script/sentence - script.csv\"\nall_data = pd.read_csv(dataset_path)\n\nif (technique == \"humor\"):\n  all_data = all_data[['Sentence', 'Humor',\"script\"]]\nelif (technique == \"hate speech\"):\n  all_data = all_data[['Sentence', 'Hate_speech', \"script\"]]\nelse:\n  all_data = all_data[['Sentence', 'Sentiment', \"script\"]]\n\nall_data.columns = ['Sentence', 'Label', \"script\"]\nall_data['Label'], uniq = pd.factorize(all_data['Label'])\n\nX = all_data[['Sentence',\"script\"]].values.tolist()\ny = all_data[['Label', \"script\"]].values.tolist()","metadata":{"id":"IrYO2ofHj9MT","execution":{"iopub.status.busy":"2023-01-07T07:09:43.325928Z","iopub.execute_input":"2023-01-07T07:09:43.326282Z","iopub.status.idle":"2023-01-07T07:09:43.453143Z","shell.execute_reply.started":"2023-01-07T07:09:43.326247Z","shell.execute_reply":"2023-01-07T07:09:43.452206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.groupby(\"script\").count()","metadata":{"execution":{"iopub.status.busy":"2023-01-07T07:09:43.454355Z","iopub.execute_input":"2023-01-07T07:09:43.454717Z","iopub.status.idle":"2023-01-07T07:09:43.478276Z","shell.execute_reply.started":"2023-01-07T07:09:43.454682Z","shell.execute_reply":"2023-01-07T07:09:43.477372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.groupby(\"Label\").count()\n","metadata":{"id":"RAGeGmAwKdBx","outputId":"84d81202-6f21-42f4-e954-fc62fe8e6017","execution":{"iopub.status.busy":"2023-01-07T07:09:43.480397Z","iopub.execute_input":"2023-01-07T07:09:43.481143Z","iopub.status.idle":"2023-01-07T07:09:43.493211Z","shell.execute_reply.started":"2023-01-07T07:09:43.481108Z","shell.execute_reply":"2023-01-07T07:09:43.491974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uniq","metadata":{"id":"fyjrjjfZlJ-w","outputId":"306671f6-1ac1-4be4-ade4-0155fa94105c","execution":{"iopub.status.busy":"2023-01-07T07:09:43.495111Z","iopub.execute_input":"2023-01-07T07:09:43.49547Z","iopub.status.idle":"2023-01-07T07:09:43.502984Z","shell.execute_reply.started":"2023-01-07T07:09:43.495435Z","shell.execute_reply":"2023-01-07T07:09:43.502027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if (technique == 'humor'):\n    num_labels=2\n    id2label={ 0: \"Non-humorous\", 1: \"Humorous\"}\n    classes = [ \"0\",\"1\",]\n#     label_words = {\"0\": [\"Non-humorous\"],\"1\": [\"Humorous\"],}\n#     label_words={\n#           \"0\": [\"unamusing\", \"lame\",\"humorless\",\"serious\",\"gloomy\",\"uncomic\"],\n#           \"1\": [\"funny\", \"comedic\", \"amusing\", \"comic\", \"comical\", \"entertaining\"],}\n    label_words={\n        \"0\": [\"නරකයි\", \"බරපතල\",\"අඳුරු\",\"භයානකයි\"],\n        \"1\": [\"හාස්‍යය\",\"හාස්‍යජනකයි\",\"විනෝදජනකයි\",\"විනෝදාත්මකයි\",\"විකට\"],}\nelif (technique == 'hate speech'):\n    num_labels=3\n    id2label={ 0: \"Not offensive\", 1: \"Hate-Inducing\", 2: \"Abusive\"}\n    classes = [ \"0\",\"1\",\"2\"]\n#     label_words={\"0\": [\"ප්‍රසන්න\",\"අහිංසක\",\"හොඳයි\"],\n#           \"1\": [\"සතුරුකම\",\"ද්වේශ\",\"අකමැති\",\"සතුරු\"],\n#           \"2\": [\"අපවාදාත්මක\",\"අසභ්‍ය\",\"නින්දා\"]}\n\n    label_words={\"0\": [\"pleasant\",\"unoffensive\",\"good\"],\n          \"1\": [\"animosity\",\"hateful\",\"dislike\",\"hostile\"],\n          \"2\": [\"abusive\",\"vulgar\",\"derogatory\"]}\nelse:\n    num_labels=4\n    id2label={ 0: \"Negative\", 1: \"Neutral\", 2: \"Positive\", 3:\"Conflict\"}\n    classes = [ \"0\",\"1\",\"2\",\"3\"]\n#     label_words={\"0\":[\"good\", \"positive\",\"great\"],\n#                  \"1\":[\"bad\", \"negative\",\"terrible\"],\n#                  \"2\":[\"okay\", \"neutral\"],\n#                  \"3\":[\"conflict\"]}\n    label_words={\"0\":[\"හොඳයි\", \"ධනාත්මක\"],\n                \"1\":[\"නරකයි\", \"ඍණාත්මක\"],\n                \"2\":[\"මධ්යස්ථ\"],\n                \"3\":[\"ගැටුම්කාරී\"]}","metadata":{"id":"x6htYjQblBDl","execution":{"iopub.status.busy":"2023-01-07T07:09:43.507797Z","iopub.execute_input":"2023-01-07T07:09:43.508354Z","iopub.status.idle":"2023-01-07T07:09:43.517225Z","shell.execute_reply.started":"2023-01-07T07:09:43.508241Z","shell.execute_reply":"2023-01-07T07:09:43.51609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = random_state)","metadata":{"id":"fqxaU1jHlDtf","execution":{"iopub.status.busy":"2023-01-07T07:09:43.51883Z","iopub.execute_input":"2023-01-07T07:09:43.519183Z","iopub.status.idle":"2023-01-07T07:09:43.53994Z","shell.execute_reply.started":"2023-01-07T07:09:43.519138Z","shell.execute_reply":"2023-01-07T07:09:43.539035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latin_examples=[]\nsinhala_examples=[]\nmixed_examples=[]\n\nfrom openprompt.data_utils import InputExample\nfor i in range(len(X_train)):\n  if(X_train[i][1]==\"Latin\"):\n      latin_examples.append(InputExample(\n            guid = i,\n            text_a =X_train[i][0],\n            label=y_train[i][0]\n        ))\n  if(X_train[i][1]==\"Sinhala\"):\n      sinhala_examples.append(InputExample(\n            guid = i,\n            text_a =X_train[i][0],\n            label=y_train[i][0]\n        ))   \n        \n  if(X_train[i][1]==\"Mixed\"):\n      mixed_examples.append(InputExample(\n            guid = i,\n            text_a =X_train[i][0],\n            label=y_train[i][0]\n        ))","metadata":{"id":"c0e7rC6HcTQj","execution":{"iopub.status.busy":"2023-01-07T07:09:43.541441Z","iopub.execute_input":"2023-01-07T07:09:43.541805Z","iopub.status.idle":"2023-01-07T07:09:43.785806Z","shell.execute_reply.started":"2023-01-07T07:09:43.541772Z","shell.execute_reply":"2023-01-07T07:09:43.784837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers.models.auto.tokenization_auto import tokenizer_class_from_name\n\nfrom openprompt.plms.utils import TokenizerWrapper\nfrom typing import List, Dict\nfrom collections import defaultdict\n\nclass MLMTokenizerWrapper(TokenizerWrapper):\n    add_input_keys = ['input_ids', 'attention_mask', 'token_type_ids']\n\n    @property\n    def mask_token(self):\n        return self.tokenizer.mask_token\n\n    @property\n    def mask_token_ids(self):\n        return self.tokenizer.mask_token_id\n\n    @property\n    def num_special_tokens_to_add(self):\n        if not hasattr(self, '_num_specials'):\n            self._num_specials = self.tokenizer.num_special_tokens_to_add()\n        return self._num_specials\n\n    def tokenize_one_example(self, wrapped_example, teacher_forcing):\n        ''' # TODO doesn't consider the situation that input has two parts\n        '''\n\n        wrapped_example, others = wrapped_example\n\n        # for some dataset like SuperGLUE.COPA, the answer requires prediction an span of\n        # the input. Or in generation tasks, we need to generate a piece of target_text.\n        # In these case, it tokenized to the encoded_tgt_text for future use.\n        encoded_tgt_text = []\n        if 'tgt_text' in others:\n            tgt_text = others['tgt_text']\n            if isinstance(tgt_text, str):\n                tgt_text = [tgt_text]\n            for t in tgt_text:\n                encoded_tgt_text.append(self.tokenizer.encode(t, add_special_tokens=False))\n\n\n        mask_id = 0 # the i-th the mask token in the template.\n\n        encoder_inputs = defaultdict(list)\n        for piece in wrapped_example:\n            if piece['loss_ids']==1:\n                if teacher_forcing: # fill the mask with the tgt task\n                    raise RuntimeError(\"Masked Language Model can't perform teacher forcing training!\")\n                else:\n                    encode_text = [self.mask_token_ids]\n                mask_id += 1\n\n            if piece['text'] in self.special_tokens_maps.keys():\n                to_replace = self.special_tokens_maps[piece['text']]\n                if to_replace is not None:\n                    piece['text'] = to_replace\n                else:\n                    raise KeyError(\"This tokenizer doesn't specify {} token.\".format(piece['text']))\n\n            if 'soft_token_ids' in piece and piece['soft_token_ids']!=0:\n                encode_text = [0] # can be replace by any token, since these token will use their own embeddings\n            else:\n                encode_text = self.tokenizer.encode(piece['text'], add_special_tokens=False)\n\n            encoding_length = len(encode_text)\n            encoder_inputs['input_ids'].append(encode_text)\n            for key in piece:\n                if key not in ['text']:\n                    encoder_inputs[key].append([piece[key]]*encoding_length)\n\n        encoder_inputs = self.truncate(encoder_inputs=encoder_inputs)\n        # delete shortenable ids\n        encoder_inputs.pop(\"shortenable_ids\")\n        encoder_inputs = self.concate_parts(input_dict=encoder_inputs)\n        encoder_inputs = self.add_special_tokens(encoder_inputs=encoder_inputs)\n        # create special input ids\n        encoder_inputs['attention_mask'] = [1] *len(encoder_inputs['input_ids'])\n        if self.create_token_type_ids:\n            encoder_inputs['token_type_ids'] = [0] *len(encoder_inputs['input_ids'])\n        # padding\n        encoder_inputs = self.padding(input_dict=encoder_inputs, max_len=self.max_seq_length, pad_id_for_inputs=self.tokenizer.pad_token_id)\n\n\n        if len(encoded_tgt_text) > 0:\n            encoder_inputs = {**encoder_inputs, \"encoded_tgt_text\": encoded_tgt_text}# convert defaultdict to dict\n        else:\n            encoder_inputs = {**encoder_inputs}\n        return encoder_inputs","metadata":{"id":"bq3XoGPBdJri","execution":{"iopub.status.busy":"2023-01-07T07:09:43.787299Z","iopub.execute_input":"2023-01-07T07:09:43.787676Z","iopub.status.idle":"2023-01-07T07:09:43.803393Z","shell.execute_reply.started":"2023-01-07T07:09:43.787639Z","shell.execute_reply":"2023-01-07T07:09:43.80222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statistics import mode\nfrom typing import List, Optional\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.tokenization_utils import PreTrainedTokenizer\nfrom transformers import BertConfig, BertTokenizer, BertModel, BertForMaskedLM, \\\n                         RobertaConfig, RobertaTokenizer, RobertaModel, RobertaForMaskedLM, \\\n                         XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaForMaskedLM\nfrom collections import namedtuple\nfrom yacs.config import CfgNode\n\nfrom openprompt.utils.logging import logger\n\n\nModelClass = namedtuple(\"ModelClass\", ('config', 'tokenizer', 'model','wrapper'))\n\n_MODEL_CLASSES = {\n    'bert': ModelClass(**{\n        'config': BertConfig,\n        'tokenizer': BertTokenizer,\n        'model':BertForMaskedLM,\n        'wrapper': MLMTokenizerWrapper,\n    }),\n    'roberta': ModelClass(**{\n        'config': RobertaConfig,\n        'tokenizer': RobertaTokenizer,\n        'model':RobertaForMaskedLM,\n        'wrapper': MLMTokenizerWrapper\n    }),\n    'xlm': ModelClass(**{\n        'config': XLMRobertaConfig,\n        'tokenizer': XLMRobertaTokenizer,\n        'model': XLMRobertaForMaskedLM,\n        'wrapper': MLMTokenizerWrapper\n    }),\n}\n\n\ndef get_model_class(plm_type: str):\n    return _MODEL_CLASSES[plm_type]\n\n\ndef load_plm(model_name, model_path, specials_to_add = None):\n    r\"\"\"A plm loader using a global config.\n    It will load the model, tokenizer, and config simulatenously.\n\n    Args:\n        config (:obj:`CfgNode`): The global config from the CfgNode.\n\n    Returns:\n        :obj:`PreTrainedModel`: The pretrained model.\n        :obj:`tokenizer`: The pretrained tokenizer.\n        :obj:`model_config`: The config of the pretrained model.\n        :obj:`wrapper`: The wrapper class of this plm.\n    \"\"\"\n    model_class = get_model_class(plm_type = model_name)\n    model_config = model_class.config.from_pretrained(model_path)\n    # you can change huggingface model_config here\n    # if 't5'  in model_name: # remove dropout according to PPT~\\ref{}\n    #     model_config.dropout_rate = 0.0\n    if 'gpt' in model_name: # add pad token for gpt\n        specials_to_add = [\"<pad>\"]\n        # model_config.attn_pdrop = 0.0\n        # model_config.resid_pdrop = 0.0\n        # model_config.embd_pdrop = 0.0\n    model = model_class.model.from_pretrained(model_path, config=model_config)\n    tokenizer = model_class.tokenizer.from_pretrained(model_path)\n    wrapper = model_class.wrapper\n\n\n    model, tokenizer = add_special_tokens(model, tokenizer, specials_to_add=specials_to_add)\n\n    if 'opt' in model_name:\n        tokenizer.add_bos_token=False\n    return model, tokenizer, model_config, wrapper\n\n\n\n\ndef load_plm_from_config(config: CfgNode):\n    r\"\"\"A plm loader using a global config.\n    It will load the model, tokenizer, and config simulatenously.\n\n    Args:\n        config (:obj:`CfgNode`): The global config from the CfgNode.\n\n    Returns:\n        :obj:`PreTrainedModel`: The pretrained model.\n        :obj:`tokenizer`: The pretrained tokenizer.\n        :obj:`model_config`: The config of the pretrained model.\n        :obj:`model_config`: The wrapper class of this plm.\n    \"\"\"\n    plm_config = config.plm\n    model_class = get_model_class(plm_type = plm_config.model_name)\n    model_config = model_class.config.from_pretrained(plm_config.model_path)\n    # you can change huggingface model_config here\n    # if 't5'  in plm_config.model_name: # remove dropout according to PPT~\\ref{}\n    #     model_config.dropout_rate = 0.0\n    if 'gpt' in plm_config.model_name: # add pad token for gpt\n        if \"<pad>\" not in config.plm.specials_to_add:\n            config.plm.specials_to_add.append(\"<pad>\")\n    model = model_class.model.from_pretrained(plm_config.model_path, config=model_config)\n    tokenizer = model_class.tokenizer.from_pretrained(plm_config.model_path)\n    wrapper = model_class.wrapper\n    model, tokenizer = add_special_tokens(model, tokenizer, specials_to_add=config.plm.specials_to_add)\n    return model, tokenizer, model_config, wrapper\n\ndef add_special_tokens(model: PreTrainedModel,\n                       tokenizer: PreTrainedTokenizer,\n                       specials_to_add: Optional[List[str]] = None):\n    r\"\"\"add the special_tokens to tokenizer if the special token\n    is not in the tokenizer.\n\n    Args:\n        model (:obj:`PreTrainedModel`): The pretrained model to resize embedding\n                after adding special tokens.\n        tokenizer (:obj:`PreTrainedTokenizer`): The pretrained tokenizer to add special tokens.\n        specials_to_add: (:obj:`List[str]`, optional): The special tokens to be added. Defaults to pad token.\n\n    Returns:\n        The resized model, The tokenizer with the added special tokens.\n\n    \"\"\"\n    if specials_to_add is None:\n        return model, tokenizer\n    for token in specials_to_add:\n        if \"pad\" in token.lower():\n            if tokenizer.pad_token is None:\n                tokenizer.add_special_tokens({'pad_token': token})\n                model.resize_token_embeddings(len(tokenizer))\n                logger.info(\"pad token is None, set to id {}\".format(tokenizer.pad_token_id))\n    return model, tokenizer","metadata":{"id":"71QIB8CjdRPI","execution":{"iopub.status.busy":"2023-01-07T07:09:43.805099Z","iopub.execute_input":"2023-01-07T07:09:43.805828Z","iopub.status.idle":"2023-01-07T07:09:43.832255Z","shell.execute_reply.started":"2023-01-07T07:09:43.805791Z","shell.execute_reply":"2023-01-07T07:09:43.831298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plm, tokenizer, model_config, WrapperClass =load_plm(\"xlm\", \"xlm-roberta-base\")","metadata":{"id":"NDPRMGSXdUwW","outputId":"c322f46e-4506-45f8-84b4-cc3804d9c171","execution":{"iopub.status.busy":"2023-01-07T07:09:43.833374Z","iopub.execute_input":"2023-01-07T07:09:43.835039Z","iopub.status.idle":"2023-01-07T07:10:30.30131Z","shell.execute_reply.started":"2023-01-07T07:09:43.835005Z","shell.execute_reply":"2023-01-07T07:10:30.300187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from openprompt.prompts import MixedTemplate\npromptTemplate_latin = MixedTemplate(\n    model=plm,\n    text = '{\"placeholder\": \"text_a\"} {\"soft\": \"It\"} {\"soft\": \"was\"} {\"mask\"}.',\n    tokenizer = tokenizer,\n)\n\npromptTemplate_sinhala = MixedTemplate(\n    model=plm,\n    text = '{\"placeholder\": \"text_a\"} {\"soft\": \"It\"} {\"soft\": \"was\"} {\"mask\"}.',\n    tokenizer = tokenizer,\n)\n\npromptTemplate_mixed = MixedTemplate(\n    model=plm,\n    text = '{\"placeholder\": \"text_a\"} {\"soft\": \"It\"} {\"soft\": \"was\"} {\"mask\"}.',\n    tokenizer = tokenizer,\n)","metadata":{"id":"jD4_Q5w0dXqx","execution":{"iopub.status.busy":"2023-01-07T07:10:30.303107Z","iopub.execute_input":"2023-01-07T07:10:30.303516Z","iopub.status.idle":"2023-01-07T07:10:30.316034Z","shell.execute_reply.started":"2023-01-07T07:10:30.303461Z","shell.execute_reply":"2023-01-07T07:10:30.314835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from openprompt.prompts import SoftVerbalizer\npromptVerbalizer_latin = SoftVerbalizer(tokenizer, plm, num_classes=num_labels)\npromptVerbalizer_sinhala = SoftVerbalizer(tokenizer, plm, num_classes=num_labels)\npromptVerbalizer_mixed = SoftVerbalizer(tokenizer, plm, num_classes=num_labels)","metadata":{"id":"TaRA_VwEdjso","execution":{"iopub.status.busy":"2023-01-07T07:10:30.318839Z","iopub.execute_input":"2023-01-07T07:10:30.319849Z","iopub.status.idle":"2023-01-07T07:10:31.940833Z","shell.execute_reply.started":"2023-01-07T07:10:30.319806Z","shell.execute_reply":"2023-01-07T07:10:31.939818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from openprompt import PromptForClassification\npromptModel_latin = PromptForClassification(\n    template = promptTemplate_latin,\n    plm = plm,\n    verbalizer = promptVerbalizer_latin,\n)\npromptModel_sinhala = PromptForClassification(\n    template = promptTemplate_sinhala,\n    plm = plm,\n    verbalizer = promptVerbalizer_sinhala,\n)\npromptModel_mixed = PromptForClassification(\n    template = promptTemplate_mixed,\n    plm = plm,\n    verbalizer = promptVerbalizer_mixed,\n)","metadata":{"id":"X4RoGcShfJlz","execution":{"iopub.status.busy":"2023-01-07T07:10:31.942722Z","iopub.execute_input":"2023-01-07T07:10:31.943164Z","iopub.status.idle":"2023-01-07T07:10:31.950923Z","shell.execute_reply.started":"2023-01-07T07:10:31.943123Z","shell.execute_reply":"2023-01-07T07:10:31.949625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 128\n\nfrom openprompt import PromptDataLoader\nlatin_data_loader = PromptDataLoader(\n    dataset = latin_examples,\n    tokenizer = tokenizer,\n    template = promptTemplate_latin,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=32,\n    max_length=MAX_LEN,\n    truncation=True,\n    padding=\"max_length\"\n)\n\nsinhala_data_loader = PromptDataLoader(\n    dataset = sinhala_examples,\n    tokenizer = tokenizer,\n    template = promptTemplate_sinhala,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=32,\n    max_length=MAX_LEN,\n    truncation=True,\n    padding=\"max_length\"\n)\n\nmixed_data_loader = PromptDataLoader(\n    dataset = mixed_examples,\n    tokenizer = tokenizer,\n    template = promptTemplate_mixed,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=32,\n    max_length=MAX_LEN,\n    truncation=True,\n    padding=\"max_length\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T07:10:31.952624Z","iopub.execute_input":"2023-01-07T07:10:31.953057Z","iopub.status.idle":"2023-01-07T07:10:40.796925Z","shell.execute_reply.started":"2023-01-07T07:10:31.95302Z","shell.execute_reply":"2023-01-07T07:10:40.795894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import  AdamW, get_linear_schedule_with_warmup\n","metadata":{"id":"_bid98egnL2r","outputId":"62c67d59-283e-48f1-db58-1bc46cc46b84","execution":{"iopub.status.busy":"2023-01-07T07:10:40.79984Z","iopub.execute_input":"2023-01-07T07:10:40.800126Z","iopub.status.idle":"2023-01-07T07:10:40.806601Z","shell.execute_reply.started":"2023-01-07T07:10:40.800099Z","shell.execute_reply":"2023-01-07T07:10:40.805542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_func(promptModel,data_loader):\n    loss_func = torch.nn.CrossEntropyLoss()\n    optimizer_grouped_parameters = [\n        {'params': [p for n,p in promptModel.template.named_parameters() if \"raw_embedding\" not in n]}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=1e-3)\n\n    optimizer_grouped_parameters1 = [\n        {'params': promptModel.verbalizer.group_parameters_1, \"lr\":3e-5},\n        {'params': promptModel.verbalizer.group_parameters_2, \"lr\":3e-4},\n    ]\n\n    optimizer1 = AdamW(optimizer_grouped_parameters1)\n    \n    \n    promptModel=promptModel.cuda()\n    for epoch in range(1):\n        tot_loss = 0\n        for step, inputs in enumerate(data_loader):\n            inputs = inputs.cuda()\n            logits = promptModel(inputs)\n            labels = inputs['label']\n            loss = loss_func(logits, labels)\n            loss.backward()\n            tot_loss += loss.item()\n            optimizer.step()\n            optimizer.zero_grad()\n            optimizer1.step()\n            optimizer1.zero_grad()","metadata":{"execution":{"iopub.status.busy":"2023-01-07T07:10:40.807962Z","iopub.execute_input":"2023-01-07T07:10:40.808368Z","iopub.status.idle":"2023-01-07T07:10:40.819283Z","shell.execute_reply.started":"2023-01-07T07:10:40.80833Z","shell.execute_reply":"2023-01-07T07:10:40.818281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_func(promptModel_latin,latin_data_loader)\ntrain_func(promptModel_sinhala,sinhala_data_loader)\ntrain_func(promptModel_mixed,mixed_data_loader)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T07:10:40.820521Z","iopub.execute_input":"2023-01-07T07:10:40.821043Z","iopub.status.idle":"2023-01-07T07:10:51.965625Z","shell.execute_reply.started":"2023-01-07T07:10:40.821009Z","shell.execute_reply":"2023-01-07T07:10:51.962936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latin_examples_test=[]\nsinhala_examples_test=[]\nmixed_examples_test=[]\nfrom openprompt.data_utils import InputExample\nfor i in range(len(X_test)):\n  if(X_test[i][1]==\"Latin\"):\n      latin_examples_test.append(InputExample(\n            guid = i,\n            text_a =X_test[i][0],\n            label=y_test[i][0]\n        ))\n  if(X_test[i][1]==\"Sinhala\"):\n      sinhala_examples_test.append(InputExample(\n            guid = i,\n            text_a =X_test[i][0],\n            label=y_test[i][0]\n        ))   \n        \n  if(X_test[i][1]==\"Mixed\"):\n      mixed_examples_test.append(InputExample(\n            guid = i,\n            text_a =X_test[i][0],\n            label=y_test[i][0]\n        ))\n    \n  ","metadata":{"id":"UaxDXhICnDOG","execution":{"iopub.status.busy":"2023-01-07T07:10:51.966784Z","iopub.status.idle":"2023-01-07T07:10:51.967745Z","shell.execute_reply.started":"2023-01-07T07:10:51.967437Z","shell.execute_reply":"2023-01-07T07:10:51.967465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"latin_data_loader = PromptDataLoader(\n    dataset = latin_examples_test,\n    tokenizer = tokenizer,\n    template = promptTemplate_latin,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=32,\n    max_length=MAX_LEN,\n    truncation=True,\n    padding=\"max_length\"\n)","metadata":{"id":"eQIWD-aHgfDQ","outputId":"2aae1828-ab3c-4d7a-ba7e-bfd1eee91ad7","execution":{"iopub.status.busy":"2023-01-07T07:10:51.969614Z","iopub.status.idle":"2023-01-07T07:10:51.970128Z","shell.execute_reply.started":"2023-01-07T07:10:51.969869Z","shell.execute_reply":"2023-01-07T07:10:51.969895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sinhala_data_loader = PromptDataLoader(\n    dataset = sinhala_examples_test,\n    tokenizer = tokenizer,\n    template = promptTemplate_sinhala,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=32,\n    max_length=MAX_LEN,\n    truncation=True,\n    padding=\"max_length\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T07:10:51.971793Z","iopub.status.idle":"2023-01-07T07:10:51.972777Z","shell.execute_reply.started":"2023-01-07T07:10:51.972505Z","shell.execute_reply":"2023-01-07T07:10:51.972533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mixed_data_loader = PromptDataLoader(\n    dataset = mixed_examples_test,\n    tokenizer = tokenizer,\n    template = promptTemplate_mixed,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=32,\n    max_length=MAX_LEN,\n    truncation=True,\n    padding=\"max_length\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T07:10:51.974375Z","iopub.status.idle":"2023-01-07T07:10:51.974929Z","shell.execute_reply.started":"2023-01-07T07:10:51.974642Z","shell.execute_reply":"2023-01-07T07:10:51.974687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def test_func(promptModel,data_loader):\n#     torch.cuda.empty_cache()\n#     promptModel=promptModel.cuda()\n#     allpreds = []\n#     alllabels = []\n#     for step, inputs in enumerate(data_loader):\n#         inputs=inputs.cuda();\n#         logits = promptModel(inputs)    \n#         labels = inputs['label']\n#         alllabels.extend(labels.cpu().tolist())\n#         allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n#     return allpreds,alllabels","metadata":{"id":"_i1RAT7DgDyV","execution":{"iopub.status.busy":"2023-01-07T07:10:51.976745Z","iopub.status.idle":"2023-01-07T07:10:51.977247Z","shell.execute_reply.started":"2023-01-07T07:10:51.976996Z","shell.execute_reply":"2023-01-07T07:10:51.977019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef compute_metrics(allpreds,alllabels):\n    metric1 = load_metric(\"precision\")\n    metric2 = load_metric(\"recall\")\n    metric3 = load_metric(\"f1\")\n    metric4 = load_metric(\"accuracy\")\n    \n    predictions, labels = allpreds,alllabels\n    precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n    recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n    f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    macro_precision = metric1.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n    macro_recall = metric2.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n    macro_f1 = metric3.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n    return {\"accuracy\":accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"macro_precision\": macro_precision, \"macro_recall\": macro_recall, \"macro_f1\": macro_f1}","metadata":{"id":"Csmm3ou8sKAp","execution":{"iopub.status.busy":"2023-01-07T07:10:51.97882Z","iopub.status.idle":"2023-01-07T07:10:51.980069Z","shell.execute_reply.started":"2023-01-07T07:10:51.979795Z","shell.execute_reply":"2023-01-07T07:10:51.979822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"allpreds,alllabels=test_func(promptModel_latin,latin_data_loader)\ncompute_metrics(allpreds,alllabels)","metadata":{"id":"RA4T07_wowtR","outputId":"376d90c8-054d-4aed-b126-d28c50009156","execution":{"iopub.status.busy":"2023-01-07T07:10:51.981439Z","iopub.status.idle":"2023-01-07T07:10:51.982478Z","shell.execute_reply.started":"2023-01-07T07:10:51.982203Z","shell.execute_reply":"2023-01-07T07:10:51.982233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"allpreds1,alllabels1=test_func(promptModel_sinhala,sinhala_data_loader)\ncompute_metrics(allpreds1,alllabels1)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T07:10:51.984533Z","iopub.status.idle":"2023-01-07T07:10:51.985044Z","shell.execute_reply.started":"2023-01-07T07:10:51.984784Z","shell.execute_reply":"2023-01-07T07:10:51.984809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"allpreds2,alllabels2=test_func(promptModel_mixed,mixed_data_loader)\ncompute_metrics(allpreds2,alllabels2)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T07:10:51.986859Z","iopub.status.idle":"2023-01-07T07:10:51.987366Z","shell.execute_reply.started":"2023-01-07T07:10:51.987105Z","shell.execute_reply":"2023-01-07T07:10:51.98713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compute_metrics(allpreds  + allpreds1 + allpreds2, alllabels + allpreds1 + allpreds2)","metadata":{"execution":{"iopub.status.busy":"2023-01-07T07:10:51.988784Z","iopub.status.idle":"2023-01-07T07:10:51.989851Z","shell.execute_reply.started":"2023-01-07T07:10:51.98956Z","shell.execute_reply":"2023-01-07T07:10:51.989591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# c=0\n# for i,j in zip(allpreds, alllabels):\n#   if(i!=j):\n#     print(c,X_test[c],\"---->\",\"predicted :\", uniq[i],\" actual :\",  uniq[j])\n\n#   c+=1","metadata":{"id":"BMfB84xM0b3h","outputId":"899612f0-2384-4e38-afa1-aee0384514d0","execution":{"iopub.status.busy":"2023-01-07T07:10:51.991678Z","iopub.status.idle":"2023-01-07T07:10:51.992178Z","shell.execute_reply.started":"2023-01-07T07:10:51.991911Z","shell.execute_reply":"2023-01-07T07:10:51.991934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# c=0\n# for i,j in zip(allpreds, alllabels):\n#   if(i==j):\n#     print(c,X_test[c],\"---->\",\"predicted :\", uniq[i],\" actual :\",  uniq[j])\n    \n#   c+=1","metadata":{"id":"d-aRTvUG9jR7","outputId":"893be599-87cd-45f8-b433-787791d6a7bf","execution":{"iopub.status.busy":"2023-01-07T07:10:51.993643Z","iopub.status.idle":"2023-01-07T07:10:51.994732Z","shell.execute_reply.started":"2023-01-07T07:10:51.994422Z","shell.execute_reply":"2023-01-07T07:10:51.994454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"zs7_5iaFDRaO"},"execution_count":null,"outputs":[]}]}