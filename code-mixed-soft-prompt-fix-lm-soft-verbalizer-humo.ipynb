{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install openprompt","metadata":{"id":"GJnzG561XzuD","outputId":"8a1c54ef-d5f9-4ac9-9b92-6dff736b9344","execution":{"iopub.status.busy":"2022-11-25T05:27:47.837815Z","iopub.execute_input":"2022-11-25T05:27:47.838731Z","iopub.status.idle":"2022-11-25T05:28:00.644437Z","shell.execute_reply.started":"2022-11-25T05:27:47.838617Z","shell.execute_reply":"2022-11-25T05:28:00.642964Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting openprompt\n  Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting rouge==1.0.0\n  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\nCollecting sentencepiece==0.1.96\n  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: yacs in /opt/conda/lib/python3.7/site-packages (from openprompt) (0.1.8)\nRequirement already satisfied: transformers>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from openprompt) (4.20.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from openprompt) (0.3.5.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from openprompt) (2.1.0)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from openprompt) (5.0.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from openprompt) (3.7)\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.7/site-packages (from openprompt) (2.5.1)\nRequirement already satisfied: tqdm>=4.62.2 in /opt/conda/lib/python3.7/site-packages (from openprompt) (4.64.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from openprompt) (1.7.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rouge==1.0.0->openprompt) (1.15.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (2021.11.10)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (21.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (1.21.6)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (4.13.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (0.12.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (0.10.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (3.7.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (2.28.1)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (2022.8.2)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (0.70.13)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (1.3.5)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (3.8.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (0.18.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (3.0.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->openprompt) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->openprompt) (1.0.1)\nRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorboardX->openprompt) (3.19.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (1.3.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (6.0.2)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (2.1.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (0.13.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (21.4.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (4.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (1.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (1.7.2)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.10.0->openprompt) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.10.0->openprompt) (2022.9.24)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.10.0->openprompt) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->openprompt) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->openprompt) (2022.1)\nInstalling collected packages: sentencepiece, rouge, openprompt\n  Attempting uninstall: sentencepiece\n    Found existing installation: sentencepiece 0.1.97\n    Uninstalling sentencepiece-0.1.97:\n      Successfully uninstalled sentencepiece-0.1.97\nSuccessfully installed openprompt-1.0.1 rouge-1.0.0 sentencepiece-0.1.96\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:28:47.994137Z","iopub.execute_input":"2022-11-25T05:28:47.994514Z","iopub.status.idle":"2022-11-25T05:28:57.744331Z","shell.execute_reply.started":"2022-11-25T05:28:47.994481Z","shell.execute_reply":"2022-11-25T05:28:57.743001Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.64.0)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.3.5.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.28.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.10.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from evaluate) (3.0.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.70.13)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.21.6)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2022.8.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.13.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.3.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (5.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (3.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->evaluate) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2022.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom transformers import TrainingArguments, Trainer\nimport evaluate\nfrom datasets import load_metric\n","metadata":{"id":"6BIRiGiWkVU6","execution":{"iopub.status.busy":"2022-11-25T05:28:57.747210Z","iopub.execute_input":"2022-11-25T05:28:57.747727Z","iopub.status.idle":"2022-11-25T05:28:59.979721Z","shell.execute_reply.started":"2022-11-25T05:28:57.747694Z","shell.execute_reply":"2022-11-25T05:28:59.978782Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model_type = \"XLM-R\" #@param [\"SinBERT\", \"Bert\", \"XLM-R\"]\ntechnique = \"humor\" #@param [\"humor\", \"hate speech\", \"sentiment\"]\nload_adapter = False #@param {type:\"boolean\"}\n# train = True #@param {type:\"boolean\"}\nunfreeze_model = False #@param {type:\"boolean\"}\nsave_adapter = False #@param {type:\"boolean\"}\noversample_dataset = False #@param {type:\"boolean\"}\nlang_adapter_setting = \"none\" #@param [\"none\", \"stack\", \"parallel\"]\nrandom_state = 42 #@param\nadapter_config = \"pfeiffer\" #@param [\"houlsby\", \"pfeiffer\"]\nover_sampling_technique = \"ROS\" #@param [\"\", \"ROS\",\"ADASYN\", \"SMOTE\", \"BorderlineSMOTE\"]\nsampling_strategy = \"1:0.25:0.25\" #@param [] {allow-input: true} \n## eg: 1:0.25:0.25 for hate | 0.5 for humor | 1:1:1:1 or 0.5:1:0.5:0.5 or 0.25:1:0.25:0.25 for sentiment","metadata":{"id":"JUCDlx1akOCt","execution":{"iopub.status.busy":"2022-11-25T05:28:21.912253Z","iopub.execute_input":"2022-11-25T05:28:21.912667Z","iopub.status.idle":"2022-11-25T05:28:21.919101Z","shell.execute_reply.started":"2022-11-25T05:28:21.912629Z","shell.execute_reply":"2022-11-25T05:28:21.917899Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from openprompt.data_utils.text_classification_dataset import PROCESSORS\nfrom datasets import load_dataset\n","metadata":{"id":"jF3gImOqX0V8","execution":{"iopub.status.busy":"2022-11-25T05:28:24.399097Z","iopub.execute_input":"2022-11-25T05:28:24.400155Z","iopub.status.idle":"2022-11-25T05:28:32.819169Z","shell.execute_reply.started":"2022-11-25T05:28:24.400109Z","shell.execute_reply":"2022-11-25T05:28:32.818132Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset_path=\"/kaggle/input/cmcs-dataset-task/ompleted_draft - ompleted_draft.csv\"\nall_data = pd.read_csv(dataset_path)\n\nif (technique == \"humor\"):\n  all_data = all_data[['Sentence', 'Humor']]\nelif (technique == \"hate speech\"):\n  all_data = all_data[['Sentence', 'Hate_speech']]\nelse:\n  all_data = all_data[['Sentence', 'Sentiment']]\n\nall_data.columns = ['Sentence', 'Label']\nall_data['Label'], uniq = pd.factorize(all_data['Label'])\n\nX = all_data['Sentence'].values.tolist()\ny = all_data['Label'].values.tolist()","metadata":{"id":"IrYO2ofHj9MT","execution":{"iopub.status.busy":"2022-11-25T05:29:17.339990Z","iopub.execute_input":"2022-11-25T05:29:17.340362Z","iopub.status.idle":"2022-11-25T05:29:17.435063Z","shell.execute_reply.started":"2022-11-25T05:29:17.340331Z","shell.execute_reply":"2022-11-25T05:29:17.434085Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"all_data.groupby(\"Label\").count()\n","metadata":{"id":"RAGeGmAwKdBx","outputId":"84d81202-6f21-42f4-e954-fc62fe8e6017","execution":{"iopub.status.busy":"2022-11-25T05:29:19.186051Z","iopub.execute_input":"2022-11-25T05:29:19.186808Z","iopub.status.idle":"2022-11-25T05:29:19.207111Z","shell.execute_reply.started":"2022-11-25T05:29:19.186769Z","shell.execute_reply":"2022-11-25T05:29:19.206211Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"       Sentence\nLabel          \n0         12213\n1          1305","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n    </tr>\n    <tr>\n      <th>Label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12213</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1305</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"uniq","metadata":{"id":"fyjrjjfZlJ-w","outputId":"306671f6-1ac1-4be4-ade4-0155fa94105c","execution":{"iopub.status.busy":"2022-11-25T05:29:22.345892Z","iopub.execute_input":"2022-11-25T05:29:22.346272Z","iopub.status.idle":"2022-11-25T05:29:22.354175Z","shell.execute_reply.started":"2022-11-25T05:29:22.346241Z","shell.execute_reply":"2022-11-25T05:29:22.353071Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Index(['Non-humorous', 'Humorous'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"if (technique == 'humor'):\n    num_labels=2\n    id2label={ 0: \"Non-humorous\", 1: \"Humorous\"}\nelif (technique == 'hate speech'):\n    num_labels=3\n    id2label={ 0: \"Not offensive\", 1: \"Hate-Inducing\", 2: \"Abusive\"}\nelse:\n    num_labels=4\n    id2label={ 0: \"Negative\", 1: \"Neutral\", 2: \"Positive\", 3:\"Conflict\"}","metadata":{"id":"x6htYjQblBDl","execution":{"iopub.status.busy":"2022-11-25T05:28:34.921071Z","iopub.execute_input":"2022-11-25T05:28:34.921495Z","iopub.status.idle":"2022-11-25T05:28:34.929097Z","shell.execute_reply.started":"2022-11-25T05:28:34.921461Z","shell.execute_reply":"2022-11-25T05:28:34.928023Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = random_state)","metadata":{"id":"fqxaU1jHlDtf","execution":{"iopub.status.busy":"2022-11-25T05:28:38.271257Z","iopub.execute_input":"2022-11-25T05:28:38.271729Z","iopub.status.idle":"2022-11-25T05:28:38.305314Z","shell.execute_reply.started":"2022-11-25T05:28:38.271689Z","shell.execute_reply":"2022-11-25T05:28:38.303944Z"},"trusted":true},"execution_count":6,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2369612312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"],"ename":"NameError","evalue":"name 'train_test_split' is not defined","output_type":"error"}]},{"cell_type":"code","source":"examples=[]\nfrom openprompt.data_utils import InputExample\nfor i in range(len(X_train)):\n  examples.append(InputExample(\n        guid = i,\n        text_a =X_train[i],\n        label=y_train[i]\n    ))","metadata":{"id":"c0e7rC6HcTQj","execution":{"iopub.status.busy":"2022-11-23T13:42:45.325897Z","iopub.status.idle":"2022-11-23T13:42:45.326675Z","shell.execute_reply.started":"2022-11-23T13:42:45.326327Z","shell.execute_reply":"2022-11-23T13:42:45.326358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers.models.auto.tokenization_auto import tokenizer_class_from_name\n\nfrom openprompt.plms.utils import TokenizerWrapper\nfrom typing import List, Dict\nfrom collections import defaultdict\n\nclass MLMTokenizerWrapper(TokenizerWrapper):\n    add_input_keys = ['input_ids', 'attention_mask', 'token_type_ids']\n\n    @property\n    def mask_token(self):\n        return self.tokenizer.mask_token\n\n    @property\n    def mask_token_ids(self):\n        return self.tokenizer.mask_token_id\n\n    @property\n    def num_special_tokens_to_add(self):\n        if not hasattr(self, '_num_specials'):\n            self._num_specials = self.tokenizer.num_special_tokens_to_add()\n        return self._num_specials\n\n    def tokenize_one_example(self, wrapped_example, teacher_forcing):\n        ''' # TODO doesn't consider the situation that input has two parts\n        '''\n\n        wrapped_example, others = wrapped_example\n\n        # for some dataset like SuperGLUE.COPA, the answer requires prediction an span of\n        # the input. Or in generation tasks, we need to generate a piece of target_text.\n        # In these case, it tokenized to the encoded_tgt_text for future use.\n        encoded_tgt_text = []\n        if 'tgt_text' in others:\n            tgt_text = others['tgt_text']\n            if isinstance(tgt_text, str):\n                tgt_text = [tgt_text]\n            for t in tgt_text:\n                encoded_tgt_text.append(self.tokenizer.encode(t, add_special_tokens=False))\n\n\n        mask_id = 0 # the i-th the mask token in the template.\n\n        encoder_inputs = defaultdict(list)\n        for piece in wrapped_example:\n            if piece['loss_ids']==1:\n                if teacher_forcing: # fill the mask with the tgt task\n                    raise RuntimeError(\"Masked Language Model can't perform teacher forcing training!\")\n                else:\n                    encode_text = [self.mask_token_ids]\n                mask_id += 1\n\n            if piece['text'] in self.special_tokens_maps.keys():\n                to_replace = self.special_tokens_maps[piece['text']]\n                if to_replace is not None:\n                    piece['text'] = to_replace\n                else:\n                    raise KeyError(\"This tokenizer doesn't specify {} token.\".format(piece['text']))\n\n            if 'soft_token_ids' in piece and piece['soft_token_ids']!=0:\n                encode_text = [0] # can be replace by any token, since these token will use their own embeddings\n            else:\n                encode_text = self.tokenizer.encode(piece['text'], add_special_tokens=False)\n\n            encoding_length = len(encode_text)\n            encoder_inputs['input_ids'].append(encode_text)\n            for key in piece:\n                if key not in ['text']:\n                    encoder_inputs[key].append([piece[key]]*encoding_length)\n\n        encoder_inputs = self.truncate(encoder_inputs=encoder_inputs)\n        # delete shortenable ids\n        encoder_inputs.pop(\"shortenable_ids\")\n        encoder_inputs = self.concate_parts(input_dict=encoder_inputs)\n        encoder_inputs = self.add_special_tokens(encoder_inputs=encoder_inputs)\n        # create special input ids\n        encoder_inputs['attention_mask'] = [1] *len(encoder_inputs['input_ids'])\n        if self.create_token_type_ids:\n            encoder_inputs['token_type_ids'] = [0] *len(encoder_inputs['input_ids'])\n        # padding\n        encoder_inputs = self.padding(input_dict=encoder_inputs, max_len=self.max_seq_length, pad_id_for_inputs=self.tokenizer.pad_token_id)\n\n\n        if len(encoded_tgt_text) > 0:\n            encoder_inputs = {**encoder_inputs, \"encoded_tgt_text\": encoded_tgt_text}# convert defaultdict to dict\n        else:\n            encoder_inputs = {**encoder_inputs}\n        return encoder_inputs","metadata":{"id":"bq3XoGPBdJri","execution":{"iopub.status.busy":"2022-11-23T13:42:45.330588Z","iopub.status.idle":"2022-11-23T13:42:45.332727Z","shell.execute_reply.started":"2022-11-23T13:42:45.332238Z","shell.execute_reply":"2022-11-23T13:42:45.332281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statistics import mode\nfrom typing import List, Optional\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.tokenization_utils import PreTrainedTokenizer\nfrom transformers import BertConfig, BertTokenizer, BertModel, BertForMaskedLM, \\\n                         RobertaConfig, RobertaTokenizer, RobertaModel, RobertaForMaskedLM, \\\n                         XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaForMaskedLM\nfrom collections import namedtuple\nfrom yacs.config import CfgNode\n\nfrom openprompt.utils.logging import logger\n\n\nModelClass = namedtuple(\"ModelClass\", ('config', 'tokenizer', 'model','wrapper'))\n\n_MODEL_CLASSES = {\n    'bert': ModelClass(**{\n        'config': BertConfig,\n        'tokenizer': BertTokenizer,\n        'model':BertForMaskedLM,\n        'wrapper': MLMTokenizerWrapper,\n    }),\n    'roberta': ModelClass(**{\n        'config': RobertaConfig,\n        'tokenizer': RobertaTokenizer,\n        'model':RobertaForMaskedLM,\n        'wrapper': MLMTokenizerWrapper\n    }),\n    'xlm': ModelClass(**{\n        'config': XLMRobertaConfig,\n        'tokenizer': XLMRobertaTokenizer,\n        'model': XLMRobertaForMaskedLM,\n        'wrapper': MLMTokenizerWrapper\n    }),\n}\n\n\ndef get_model_class(plm_type: str):\n    return _MODEL_CLASSES[plm_type]\n\n\ndef load_plm(model_name, model_path, specials_to_add = None):\n    r\"\"\"A plm loader using a global config.\n    It will load the model, tokenizer, and config simulatenously.\n\n    Args:\n        config (:obj:`CfgNode`): The global config from the CfgNode.\n\n    Returns:\n        :obj:`PreTrainedModel`: The pretrained model.\n        :obj:`tokenizer`: The pretrained tokenizer.\n        :obj:`model_config`: The config of the pretrained model.\n        :obj:`wrapper`: The wrapper class of this plm.\n    \"\"\"\n    model_class = get_model_class(plm_type = model_name)\n    model_config = model_class.config.from_pretrained(model_path)\n    # you can change huggingface model_config here\n    # if 't5'  in model_name: # remove dropout according to PPT~\\ref{}\n    #     model_config.dropout_rate = 0.0\n    if 'gpt' in model_name: # add pad token for gpt\n        specials_to_add = [\"<pad>\"]\n        # model_config.attn_pdrop = 0.0\n        # model_config.resid_pdrop = 0.0\n        # model_config.embd_pdrop = 0.0\n    model = model_class.model.from_pretrained(model_path, config=model_config)\n    tokenizer = model_class.tokenizer.from_pretrained(model_path)\n    wrapper = model_class.wrapper\n\n\n    model, tokenizer = add_special_tokens(model, tokenizer, specials_to_add=specials_to_add)\n\n    if 'opt' in model_name:\n        tokenizer.add_bos_token=False\n    return model, tokenizer, model_config, wrapper\n\n\n\n\ndef load_plm_from_config(config: CfgNode):\n    r\"\"\"A plm loader using a global config.\n    It will load the model, tokenizer, and config simulatenously.\n\n    Args:\n        config (:obj:`CfgNode`): The global config from the CfgNode.\n\n    Returns:\n        :obj:`PreTrainedModel`: The pretrained model.\n        :obj:`tokenizer`: The pretrained tokenizer.\n        :obj:`model_config`: The config of the pretrained model.\n        :obj:`model_config`: The wrapper class of this plm.\n    \"\"\"\n    plm_config = config.plm\n    model_class = get_model_class(plm_type = plm_config.model_name)\n    model_config = model_class.config.from_pretrained(plm_config.model_path)\n    # you can change huggingface model_config here\n    # if 't5'  in plm_config.model_name: # remove dropout according to PPT~\\ref{}\n    #     model_config.dropout_rate = 0.0\n    if 'gpt' in plm_config.model_name: # add pad token for gpt\n        if \"<pad>\" not in config.plm.specials_to_add:\n            config.plm.specials_to_add.append(\"<pad>\")\n    model = model_class.model.from_pretrained(plm_config.model_path, config=model_config)\n    tokenizer = model_class.tokenizer.from_pretrained(plm_config.model_path)\n    wrapper = model_class.wrapper\n    model, tokenizer = add_special_tokens(model, tokenizer, specials_to_add=config.plm.specials_to_add)\n    return model, tokenizer, model_config, wrapper\n\ndef add_special_tokens(model: PreTrainedModel,\n                       tokenizer: PreTrainedTokenizer,\n                       specials_to_add: Optional[List[str]] = None):\n    r\"\"\"add the special_tokens to tokenizer if the special token\n    is not in the tokenizer.\n\n    Args:\n        model (:obj:`PreTrainedModel`): The pretrained model to resize embedding\n                after adding special tokens.\n        tokenizer (:obj:`PreTrainedTokenizer`): The pretrained tokenizer to add special tokens.\n        specials_to_add: (:obj:`List[str]`, optional): The special tokens to be added. Defaults to pad token.\n\n    Returns:\n        The resized model, The tokenizer with the added special tokens.\n\n    \"\"\"\n    if specials_to_add is None:\n        return model, tokenizer\n    for token in specials_to_add:\n        if \"pad\" in token.lower():\n            if tokenizer.pad_token is None:\n                tokenizer.add_special_tokens({'pad_token': token})\n                model.resize_token_embeddings(len(tokenizer))\n                logger.info(\"pad token is None, set to id {}\".format(tokenizer.pad_token_id))\n    return model, tokenizer","metadata":{"id":"71QIB8CjdRPI","execution":{"iopub.status.busy":"2022-11-23T13:42:45.335788Z","iopub.status.idle":"2022-11-23T13:42:45.338671Z","shell.execute_reply.started":"2022-11-23T13:42:45.338244Z","shell.execute_reply":"2022-11-23T13:42:45.338287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plm, tokenizer, model_config, WrapperClass =load_plm(\"xlm\", \"xlm-roberta-base\")","metadata":{"id":"NDPRMGSXdUwW","outputId":"c322f46e-4506-45f8-84b4-cc3804d9c171","execution":{"iopub.status.busy":"2022-11-23T13:42:45.341358Z","iopub.status.idle":"2022-11-23T13:42:45.342571Z","shell.execute_reply.started":"2022-11-23T13:42:45.342209Z","shell.execute_reply":"2022-11-23T13:42:45.342247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from openprompt.prompts import MixedTemplate\npromptTemplate = MixedTemplate(\n    model=plm,\n    text = '{\"placeholder\": \"text_a\"} {\"soft\": \"It\"} {\"soft\": \"was\"} {\"mask\"}.',\n    tokenizer = tokenizer,\n)","metadata":{"id":"jD4_Q5w0dXqx","execution":{"iopub.status.busy":"2022-11-23T13:42:45.345301Z","iopub.status.idle":"2022-11-23T13:42:45.351734Z","shell.execute_reply.started":"2022-11-23T13:42:45.351256Z","shell.execute_reply":"2022-11-23T13:42:45.351306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = [ \n    \"0\",\n    \"1\",\n]","metadata":{"id":"DrH_VvXgdgZ4","execution":{"iopub.status.busy":"2022-11-23T13:42:45.353829Z","iopub.status.idle":"2022-11-23T13:42:45.354716Z","shell.execute_reply.started":"2022-11-23T13:42:45.354372Z","shell.execute_reply":"2022-11-23T13:42:45.354405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from openprompt.prompts import SoftVerbalizer\npromptVerbalizer = SoftVerbalizer(tokenizer, plm, num_classes=2)","metadata":{"id":"TaRA_VwEdjso","execution":{"iopub.status.busy":"2022-11-23T13:42:45.357254Z","iopub.status.idle":"2022-11-23T13:42:45.358081Z","shell.execute_reply.started":"2022-11-23T13:42:45.357656Z","shell.execute_reply":"2022-11-23T13:42:45.357693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from openprompt import PromptForClassification\npromptModel = PromptForClassification(\n    template = promptTemplate,\n    plm = plm,\n    verbalizer = promptVerbalizer,\n)","metadata":{"id":"X4RoGcShfJlz","execution":{"iopub.status.busy":"2022-11-23T13:42:45.365897Z","iopub.status.idle":"2022-11-23T13:42:45.366730Z","shell.execute_reply.started":"2022-11-23T13:42:45.366339Z","shell.execute_reply":"2022-11-23T13:42:45.366373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nMAX_LEN = 128\n\nfrom openprompt import PromptDataLoader\ndata_loader = PromptDataLoader(\n    dataset = examples,\n    tokenizer = tokenizer,\n    template = promptTemplate,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=1,\n    max_length=MAX_LEN,\n    truncation=True,\n    padding=\"max_length\"\n)","metadata":{"id":"7wS6PECof2NN","outputId":"de5284af-6084-4395-94a6-197ab149a675","execution":{"iopub.status.busy":"2022-11-23T13:42:45.368723Z","iopub.status.idle":"2022-11-23T13:42:45.369549Z","shell.execute_reply.started":"2022-11-23T13:42:45.369205Z","shell.execute_reply":"2022-11-23T13:42:45.369238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import  AdamW, get_linear_schedule_with_warmup\nloss_func = torch.nn.CrossEntropyLoss()\noptimizer_grouped_parameters = [\n    {'params': [p for n,p in promptModel.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer = AdamW(optimizer_grouped_parameters, lr=1e-3)\n\noptimizer_grouped_parameters1 = [\n    {'params': promptModel.verbalizer.group_parameters_1, \"lr\":3e-5},\n    {'params': promptModel.verbalizer.group_parameters_2, \"lr\":3e-4},\n]\n\noptimizer1 = AdamW(optimizer_grouped_parameters1)\n","metadata":{"id":"_bid98egnL2r","outputId":"62c67d59-283e-48f1-db58-1bc46cc46b84","execution":{"iopub.status.busy":"2022-11-23T13:42:45.371862Z","iopub.status.idle":"2022-11-23T13:42:45.373627Z","shell.execute_reply.started":"2022-11-23T13:42:45.373237Z","shell.execute_reply":"2022-11-23T13:42:45.373277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"promptModel=promptModel.cuda()\nfor epoch in range(2):\n    tot_loss = 0\n    for step, inputs in enumerate(data_loader):\n        inputs = inputs.cuda()\n        logits = promptModel(inputs)\n        labels = inputs['label']\n        loss = loss_func(logits, labels)\n        loss.backward()\n        tot_loss += loss.item()\n        optimizer.step()\n        optimizer.zero_grad()\n        optimizer1.step()\n        optimizer1.zero_grad()\n        print(tot_loss/(step+1))","metadata":{"id":"g0jRdcVPpB8z","outputId":"5b170084-f552-423f-9b57-37ee03e2b673","execution":{"iopub.status.busy":"2022-11-23T13:42:45.376371Z","iopub.status.idle":"2022-11-23T13:42:45.379723Z","shell.execute_reply.started":"2022-11-23T13:42:45.379250Z","shell.execute_reply":"2022-11-23T13:42:45.379296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"examples_test=[]\nfrom openprompt.data_utils import InputExample\nfor i in range(len(X_test)):\n  examples_test.append(InputExample(\n        guid = i,\n        text_a =X_test[i],\n        label=y_test[i]\n    ))\n  ","metadata":{"id":"UaxDXhICnDOG","execution":{"iopub.status.busy":"2022-11-23T13:42:45.381840Z","iopub.status.idle":"2022-11-23T13:42:45.384661Z","shell.execute_reply.started":"2022-11-23T13:42:45.384247Z","shell.execute_reply":"2022-11-23T13:42:45.384291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_loader = PromptDataLoader(\n    dataset = examples_test,\n    tokenizer = tokenizer,\n    template = promptTemplate,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=1,\n    max_length=MAX_LEN,\n    truncation=True,\n    padding=\"max_length\"\n)","metadata":{"id":"eQIWD-aHgfDQ","outputId":"2aae1828-ab3c-4d7a-ba7e-bfd1eee91ad7","execution":{"iopub.status.busy":"2022-11-23T13:42:45.388240Z","iopub.status.idle":"2022-11-23T13:42:45.390675Z","shell.execute_reply.started":"2022-11-23T13:42:45.390243Z","shell.execute_reply":"2022-11-23T13:42:45.390288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\nallpreds = []\nalllabels = []\npromptModel=promptModel.cuda()\nfor step, inputs in enumerate(data_loader):\n    inputs=inputs.cuda();\n    logits = promptModel(inputs)    \n    labels = inputs['label']\n    alllabels.extend(labels.cpu().tolist())\n    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n","metadata":{"id":"_i1RAT7DgDyV","execution":{"iopub.status.busy":"2022-11-23T13:42:45.394429Z","iopub.status.idle":"2022-11-23T13:42:45.400762Z","shell.execute_reply.started":"2022-11-23T13:42:45.400284Z","shell.execute_reply":"2022-11-23T13:42:45.400334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef compute_metrics(allpreds,alllabels):\n    metric1 = load_metric(\"precision\")\n    metric2 = load_metric(\"recall\")\n    metric3 = load_metric(\"f1\")\n    metric4 = load_metric(\"accuracy\")\n    \n    predictions, labels = allpreds,alllabels\n    precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n    recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n    f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    macro_precision = metric1.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n    macro_recall = metric2.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n    macro_f1 = metric3.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n    return {\"accuracy\":accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"macro_precision\": macro_precision, \"macro_recall\": macro_recall, \"macro_f1\": macro_f1}","metadata":{"id":"Csmm3ou8sKAp","execution":{"iopub.status.busy":"2022-11-23T13:42:45.402949Z","iopub.status.idle":"2022-11-23T13:42:45.404057Z","shell.execute_reply.started":"2022-11-23T13:42:45.403696Z","shell.execute_reply":"2022-11-23T13:42:45.403730Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compute_metrics(allpreds,alllabels)","metadata":{"id":"RA4T07_wowtR","outputId":"376d90c8-054d-4aed-b126-d28c50009156","execution":{"iopub.status.busy":"2022-11-23T13:42:45.406864Z","iopub.status.idle":"2022-11-23T13:42:45.407856Z","shell.execute_reply.started":"2022-11-23T13:42:45.407507Z","shell.execute_reply":"2022-11-23T13:42:45.407544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c=0\nfor i,j in zip(allpreds, alllabels):\n  if(i!=j):\n    print(c,X_test[c],\"---->\",\"predicted :\", uniq[i],\" actual :\",  uniq[j])\n\n  c+=1","metadata":{"id":"BMfB84xM0b3h","outputId":"899612f0-2384-4e38-afa1-aee0384514d0","execution":{"iopub.status.busy":"2022-11-23T13:42:45.410468Z","iopub.status.idle":"2022-11-23T13:42:45.414814Z","shell.execute_reply.started":"2022-11-23T13:42:45.414287Z","shell.execute_reply":"2022-11-23T13:42:45.414341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c=0\nfor i,j in zip(allpreds, alllabels):\n  if(i==j):\n    print(c,X_test[c],\"---->\",\"predicted :\", uniq[i],\" actual :\",  uniq[j])\n    \n  c+=1","metadata":{"id":"d-aRTvUG9jR7","outputId":"893be599-87cd-45f8-b433-787791d6a7bf","execution":{"iopub.status.busy":"2022-11-23T13:42:45.416858Z","iopub.status.idle":"2022-11-23T13:42:45.417624Z","shell.execute_reply.started":"2022-11-23T13:42:45.417263Z","shell.execute_reply":"2022-11-23T13:42:45.417296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"zs7_5iaFDRaO"},"execution_count":null,"outputs":[]}]}