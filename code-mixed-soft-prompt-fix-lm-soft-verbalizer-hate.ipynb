{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install openprompt","metadata":{"id":"GJnzG561XzuD","outputId":"8a1c54ef-d5f9-4ac9-9b92-6dff736b9344","execution":{"iopub.status.busy":"2022-11-25T05:38:18.371467Z","iopub.execute_input":"2022-11-25T05:38:18.371939Z","iopub.status.idle":"2022-11-25T05:38:28.485297Z","shell.execute_reply.started":"2022-11-25T05:38:18.371904Z","shell.execute_reply":"2022-11-25T05:38:28.483767Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Requirement already satisfied: openprompt in /opt/conda/lib/python3.7/site-packages (1.0.1)\nRequirement already satisfied: rouge==1.0.0 in /opt/conda/lib/python3.7/site-packages (from openprompt) (1.0.0)\nRequirement already satisfied: transformers>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from openprompt) (4.20.1)\nRequirement already satisfied: tqdm>=4.62.2 in /opt/conda/lib/python3.7/site-packages (from openprompt) (4.64.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from openprompt) (1.7.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from openprompt) (3.7)\nRequirement already satisfied: sentencepiece==0.1.96 in /opt/conda/lib/python3.7/site-packages (from openprompt) (0.1.96)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from openprompt) (0.3.5.1)\nRequirement already satisfied: yacs in /opt/conda/lib/python3.7/site-packages (from openprompt) (0.1.8)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from openprompt) (5.0.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from openprompt) (2.1.0)\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.7/site-packages (from openprompt) (2.5.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rouge==1.0.0->openprompt) (1.15.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (21.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (0.10.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (4.13.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (6.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (1.21.6)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (2021.11.10)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (2.28.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.10.0->openprompt) (0.12.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (3.0.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (2022.8.2)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (0.70.13)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (3.8.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (1.3.5)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets->openprompt) (0.18.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->openprompt) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->openprompt) (8.0.4)\nRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorboardX->openprompt) (3.19.4)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (4.1.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (1.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (1.3.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (6.0.2)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (2.1.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (0.13.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (21.4.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (1.7.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->openprompt) (4.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.10.0->openprompt) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.10.0->openprompt) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers>=4.10.0->openprompt) (3.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.10.0->openprompt) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->openprompt) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->openprompt) (2022.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install evaluate\n","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:38:59.025860Z","iopub.execute_input":"2022-11-25T05:38:59.026283Z","iopub.status.idle":"2022-11-25T05:39:10.135536Z","shell.execute_reply.started":"2022-11-25T05:38:59.026248Z","shell.execute_reply":"2022-11-25T05:39:10.134047Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.7/site-packages (0.3.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.28.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from evaluate) (21.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.3.5)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.10.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from evaluate) (3.0.0)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.13.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.64.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.70.13)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2022.8.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.21.6)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.3.5.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (5.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.7.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2.1.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->evaluate) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2022.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nfrom transformers import TrainingArguments, Trainer\nimport evaluate\nfrom datasets import load_metric\n","metadata":{"id":"6BIRiGiWkVU6","execution":{"iopub.status.busy":"2022-11-25T05:39:10.138315Z","iopub.execute_input":"2022-11-25T05:39:10.138750Z","iopub.status.idle":"2022-11-25T05:39:10.144473Z","shell.execute_reply.started":"2022-11-25T05:39:10.138709Z","shell.execute_reply":"2022-11-25T05:39:10.143311Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model_type = \"XLM-R\" #@param [\"SinBERT\", \"Bert\", \"XLM-R\"]\ntechnique = \"hate speech\" #@param [\"humor\", \"hate speech\", \"sentiment\"]\nload_adapter = False #@param {type:\"boolean\"}\n# train = True #@param {type:\"boolean\"}\nunfreeze_model = False #@param {type:\"boolean\"}\nsave_adapter = False #@param {type:\"boolean\"}\noversample_dataset = True #@param {type:\"boolean\"}\nlang_adapter_setting = \"none\" #@param [\"none\", \"stack\", \"parallel\"]\nrandom_state = 42 #@param\nadapter_config = \"pfeiffer\" #@param [\"houlsby\", \"pfeiffer\"]\nover_sampling_technique = \"ROS\" #@param [\"\", \"ROS\",\"ADASYN\", \"SMOTE\", \"BorderlineSMOTE\"]\nsampling_strategy = \"1:0.25:0.25\" #@param [] {allow-input: true} \n## eg: 1:0.25:0.25 for hate | 0.5 for humor | 1:1:1:1 or 0.5:1:0.5:0.5 or 0.25:1:0.25:0.25 for sentiment","metadata":{"id":"JUCDlx1akOCt","execution":{"iopub.status.busy":"2022-11-25T05:39:13.559205Z","iopub.execute_input":"2022-11-25T05:39:13.559575Z","iopub.status.idle":"2022-11-25T05:39:13.566810Z","shell.execute_reply.started":"2022-11-25T05:39:13.559543Z","shell.execute_reply":"2022-11-25T05:39:13.565714Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from openprompt.data_utils.text_classification_dataset import PROCESSORS\nfrom datasets import load_dataset\n","metadata":{"id":"jF3gImOqX0V8","execution":{"iopub.status.busy":"2022-11-25T05:39:15.954299Z","iopub.execute_input":"2022-11-25T05:39:15.955429Z","iopub.status.idle":"2022-11-25T05:39:15.964864Z","shell.execute_reply.started":"2022-11-25T05:39:15.955376Z","shell.execute_reply":"2022-11-25T05:39:15.960615Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"dataset_path=\"/kaggle/input/cmcs-dataset-task/ompleted_draft - ompleted_draft.csv\"\nall_data = pd.read_csv(dataset_path)\n\nif (technique == \"humor\"):\n  all_data = all_data[['Sentence', 'Humor']]\nelif (technique == \"hate speech\"):\n  all_data = all_data[['Sentence', 'Hate_speech']]\nelse:\n  all_data = all_data[['Sentence', 'Sentiment']]\n\nall_data.columns = ['Sentence', 'Label']\nall_data['Label'], uniq = pd.factorize(all_data['Label'])\n\nX = all_data['Sentence'].values.tolist()\ny = all_data['Label'].values.tolist()","metadata":{"id":"IrYO2ofHj9MT","execution":{"iopub.status.busy":"2022-11-25T05:39:21.031219Z","iopub.execute_input":"2022-11-25T05:39:21.031721Z","iopub.status.idle":"2022-11-25T05:39:21.107723Z","shell.execute_reply.started":"2022-11-25T05:39:21.031674Z","shell.execute_reply":"2022-11-25T05:39:21.106722Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"all_data.groupby(\"Label\").count()\n","metadata":{"id":"RAGeGmAwKdBx","outputId":"84d81202-6f21-42f4-e954-fc62fe8e6017","execution":{"iopub.status.busy":"2022-11-25T05:39:22.379169Z","iopub.execute_input":"2022-11-25T05:39:22.379648Z","iopub.status.idle":"2022-11-25T05:39:22.403714Z","shell.execute_reply.started":"2022-11-25T05:39:22.379600Z","shell.execute_reply":"2022-11-25T05:39:22.402834Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"       Sentence\nLabel          \n0         12262\n1           348\n2           908","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence</th>\n    </tr>\n    <tr>\n      <th>Label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12262</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>348</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>908</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"uniq","metadata":{"id":"fyjrjjfZlJ-w","outputId":"306671f6-1ac1-4be4-ade4-0155fa94105c","execution":{"iopub.status.busy":"2022-11-25T05:39:25.343951Z","iopub.execute_input":"2022-11-25T05:39:25.344477Z","iopub.status.idle":"2022-11-25T05:39:25.352929Z","shell.execute_reply.started":"2022-11-25T05:39:25.344415Z","shell.execute_reply":"2022-11-25T05:39:25.351725Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"Index(['Not offensive', 'Hate-Inducing', 'Abusive'], dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:39:28.045839Z","iopub.execute_input":"2022-11-25T05:39:28.046570Z","iopub.status.idle":"2022-11-25T05:39:28.058084Z","shell.execute_reply.started":"2022-11-25T05:39:28.046508Z","shell.execute_reply":"2022-11-25T05:39:28.056632Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"if (technique == 'humor'):\n    num_labels=2\n    id2label={ 0: \"Non-humorous\", 1: \"Humorous\"}\nelif (technique == 'hate speech'):\n    num_labels=3\n    id2label={ 0: \"Not offensive\", 1: \"Hate-Inducing\", 2: \"Abusive\"}\nelse:\n    num_labels=4\n    id2label={ 0: \"Negative\", 1: \"Neutral\", 2: \"Positive\", 3:\"Conflict\"}","metadata":{"id":"x6htYjQblBDl","execution":{"iopub.status.busy":"2022-11-25T05:39:30.083323Z","iopub.execute_input":"2022-11-25T05:39:30.083685Z","iopub.status.idle":"2022-11-25T05:39:30.090380Z","shell.execute_reply.started":"2022-11-25T05:39:30.083653Z","shell.execute_reply":"2022-11-25T05:39:30.088956Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def apply_oversampling(x, y):\n\n  (unique, counts) = np.unique(y, axis=0, return_counts=True)\n  print(\"Class Distribution Without Oversampling\", counts)\n\n  # define oversampling strategy\n  if (over_sampling_technique == \"\"):\n    return x, y\n  elif (over_sampling_technique == \"ROS\"):\n    if (technique==\"humor\"):\n      oversample = RandomOverSampler(sampling_strategy = float(sampling_strategy))\n    elif (technique==\"hate speech\"):\n      sampling_ratio = sampling_strategy.split(\":\");\n      oversample = RandomOverSampler(sampling_strategy = {\n          0:int(counts[0]*float(sampling_ratio[0])), \n          1:int(counts[0]*float(sampling_ratio[1])), \n          2:int(counts[0]*float(sampling_ratio[2]))\n          })\n    elif (technique==\"sentiment\"):\n      sampling_ratio = sampling_strategy.split(\":\");\n      oversample = RandomOverSampler(sampling_strategy = {\n          0:int(counts[1]*float(sampling_ratio[0])), \n          1:int(counts[1]*float(sampling_ratio[1])), \n          2:int(counts[1]*float(sampling_ratio[2])),\n          3:int(counts[1]*float(sampling_ratio[3]))\n          })\n  elif (over_sampling_technique == \"ADASYN\"):\n    oversample = ADASYN(sampling_strategy=\"minority\")\n  elif (over_sampling_technique == \"SMOTE\"):\n    oversample = SMOTE()\n  elif (over_sampling_technique == \"BorderlineSMOTE\"):\n    oversample = BorderlineSMOTE()\n\n  # fit and apply the transform\n  X_over, y_over = oversample.fit_resample(x, y)\n\n  (unique, counts) = np.unique(y_over, axis=0, return_counts=True)\n  print(\"Class Distribution After Oversampling\", counts)\n\n  return X_over, y_over","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:39:32.982701Z","iopub.execute_input":"2022-11-25T05:39:32.983528Z","iopub.status.idle":"2022-11-25T05:39:32.997546Z","shell.execute_reply.started":"2022-11-25T05:39:32.983469Z","shell.execute_reply":"2022-11-25T05:39:32.996486Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = random_state)","metadata":{"id":"fqxaU1jHlDtf","execution":{"iopub.status.busy":"2022-11-25T05:39:33.746065Z","iopub.execute_input":"2022-11-25T05:39:33.746458Z","iopub.status.idle":"2022-11-25T05:39:33.758025Z","shell.execute_reply.started":"2022-11-25T05:39:33.746424Z","shell.execute_reply":"2022-11-25T05:39:33.756838Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"if oversample_dataset:\n  # apply oversampling\n  X_train = np.array(X_train).reshape(-1, 1)\n  X_train, y_train = apply_oversampling(X_train, y_train)\n  X_train = [x[0] for x in X_train.tolist()]","metadata":{"execution":{"iopub.status.busy":"2022-11-25T05:39:36.622128Z","iopub.execute_input":"2022-11-25T05:39:36.622587Z","iopub.status.idle":"2022-11-25T05:39:36.965277Z","shell.execute_reply.started":"2022-11-25T05:39:36.622544Z","shell.execute_reply":"2022-11-25T05:39:36.964142Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Class Distribution Without Oversampling [11036   314   816]\nClass Distribution After Oversampling [11036  2759  2759]\n","output_type":"stream"}]},{"cell_type":"code","source":"examples=[]\nfrom openprompt.data_utils import InputExample\nfor i in range(len(X_train)):\n  examples.append(InputExample(\n        guid = i,\n        text_a =X_train[i],\n        label=y_train[i]\n    ))","metadata":{"id":"c0e7rC6HcTQj","execution":{"iopub.status.busy":"2022-11-25T05:36:37.404691Z","iopub.execute_input":"2022-11-25T05:36:37.405499Z","iopub.status.idle":"2022-11-25T05:36:37.430965Z","shell.execute_reply.started":"2022-11-25T05:36:37.405459Z","shell.execute_reply":"2022-11-25T05:36:37.430047Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\nfrom transformers.models.auto.tokenization_auto import tokenizer_class_from_name\n\nfrom openprompt.plms.utils import TokenizerWrapper\nfrom typing import List, Dict\nfrom collections import defaultdict\n\nclass MLMTokenizerWrapper(TokenizerWrapper):\n    add_input_keys = ['input_ids', 'attention_mask', 'token_type_ids']\n\n    @property\n    def mask_token(self):\n        return self.tokenizer.mask_token\n\n    @property\n    def mask_token_ids(self):\n        return self.tokenizer.mask_token_id\n\n    @property\n    def num_special_tokens_to_add(self):\n        if not hasattr(self, '_num_specials'):\n            self._num_specials = self.tokenizer.num_special_tokens_to_add()\n        return self._num_specials\n\n    def tokenize_one_example(self, wrapped_example, teacher_forcing):\n        ''' # TODO doesn't consider the situation that input has two parts\n        '''\n\n        wrapped_example, others = wrapped_example\n\n        # for some dataset like SuperGLUE.COPA, the answer requires prediction an span of\n        # the input. Or in generation tasks, we need to generate a piece of target_text.\n        # In these case, it tokenized to the encoded_tgt_text for future use.\n        encoded_tgt_text = []\n        if 'tgt_text' in others:\n            tgt_text = others['tgt_text']\n            if isinstance(tgt_text, str):\n                tgt_text = [tgt_text]\n            for t in tgt_text:\n                encoded_tgt_text.append(self.tokenizer.encode(t, add_special_tokens=False))\n\n\n        mask_id = 0 # the i-th the mask token in the template.\n\n        encoder_inputs = defaultdict(list)\n        for piece in wrapped_example:\n            if piece['loss_ids']==1:\n                if teacher_forcing: # fill the mask with the tgt task\n                    raise RuntimeError(\"Masked Language Model can't perform teacher forcing training!\")\n                else:\n                    encode_text = [self.mask_token_ids]\n                mask_id += 1\n\n            if piece['text'] in self.special_tokens_maps.keys():\n                to_replace = self.special_tokens_maps[piece['text']]\n                if to_replace is not None:\n                    piece['text'] = to_replace\n                else:\n                    raise KeyError(\"This tokenizer doesn't specify {} token.\".format(piece['text']))\n\n            if 'soft_token_ids' in piece and piece['soft_token_ids']!=0:\n                encode_text = [0] # can be replace by any token, since these token will use their own embeddings\n            else:\n                encode_text = self.tokenizer.encode(piece['text'], add_special_tokens=False)\n\n            encoding_length = len(encode_text)\n            encoder_inputs['input_ids'].append(encode_text)\n            for key in piece:\n                if key not in ['text']:\n                    encoder_inputs[key].append([piece[key]]*encoding_length)\n\n        encoder_inputs = self.truncate(encoder_inputs=encoder_inputs)\n        # delete shortenable ids\n        encoder_inputs.pop(\"shortenable_ids\")\n        encoder_inputs = self.concate_parts(input_dict=encoder_inputs)\n        encoder_inputs = self.add_special_tokens(encoder_inputs=encoder_inputs)\n        # create special input ids\n        encoder_inputs['attention_mask'] = [1] *len(encoder_inputs['input_ids'])\n        if self.create_token_type_ids:\n            encoder_inputs['token_type_ids'] = [0] *len(encoder_inputs['input_ids'])\n        # padding\n        encoder_inputs = self.padding(input_dict=encoder_inputs, max_len=self.max_seq_length, pad_id_for_inputs=self.tokenizer.pad_token_id)\n\n\n        if len(encoded_tgt_text) > 0:\n            encoder_inputs = {**encoder_inputs, \"encoded_tgt_text\": encoded_tgt_text}# convert defaultdict to dict\n        else:\n            encoder_inputs = {**encoder_inputs}\n        return encoder_inputs","metadata":{"id":"bq3XoGPBdJri","execution":{"iopub.status.busy":"2022-11-25T05:36:37.433714Z","iopub.execute_input":"2022-11-25T05:36:37.434340Z","iopub.status.idle":"2022-11-25T05:36:37.450514Z","shell.execute_reply.started":"2022-11-25T05:36:37.434303Z","shell.execute_reply":"2022-11-25T05:36:37.449616Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from statistics import mode\nfrom typing import List, Optional\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.tokenization_utils import PreTrainedTokenizer\nfrom transformers import BertConfig, BertTokenizer, BertModel, BertForMaskedLM, \\\n                         RobertaConfig, RobertaTokenizer, RobertaModel, RobertaForMaskedLM, \\\n                         XLMRobertaConfig, XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaForMaskedLM\nfrom collections import namedtuple\nfrom yacs.config import CfgNode\n\nfrom openprompt.utils.logging import logger\n\n\nModelClass = namedtuple(\"ModelClass\", ('config', 'tokenizer', 'model','wrapper'))\n\n_MODEL_CLASSES = {\n    'bert': ModelClass(**{\n        'config': BertConfig,\n        'tokenizer': BertTokenizer,\n        'model':BertForMaskedLM,\n        'wrapper': MLMTokenizerWrapper,\n    }),\n    'roberta': ModelClass(**{\n        'config': RobertaConfig,\n        'tokenizer': RobertaTokenizer,\n        'model':RobertaForMaskedLM,\n        'wrapper': MLMTokenizerWrapper\n    }),\n    'xlm': ModelClass(**{\n        'config': XLMRobertaConfig,\n        'tokenizer': XLMRobertaTokenizer,\n        'model': XLMRobertaForMaskedLM,\n        'wrapper': MLMTokenizerWrapper\n    }),\n}\n\n\ndef get_model_class(plm_type: str):\n    return _MODEL_CLASSES[plm_type]\n\n\ndef load_plm(model_name, model_path, specials_to_add = None):\n    r\"\"\"A plm loader using a global config.\n    It will load the model, tokenizer, and config simulatenously.\n\n    Args:\n        config (:obj:`CfgNode`): The global config from the CfgNode.\n\n    Returns:\n        :obj:`PreTrainedModel`: The pretrained model.\n        :obj:`tokenizer`: The pretrained tokenizer.\n        :obj:`model_config`: The config of the pretrained model.\n        :obj:`wrapper`: The wrapper class of this plm.\n    \"\"\"\n    model_class = get_model_class(plm_type = model_name)\n    model_config = model_class.config.from_pretrained(model_path)\n    # you can change huggingface model_config here\n    # if 't5'  in model_name: # remove dropout according to PPT~\\ref{}\n    #     model_config.dropout_rate = 0.0\n    if 'gpt' in model_name: # add pad token for gpt\n        specials_to_add = [\"<pad>\"]\n        # model_config.attn_pdrop = 0.0\n        # model_config.resid_pdrop = 0.0\n        # model_config.embd_pdrop = 0.0\n    model = model_class.model.from_pretrained(model_path, config=model_config)\n    tokenizer = model_class.tokenizer.from_pretrained(model_path)\n    wrapper = model_class.wrapper\n\n\n    model, tokenizer = add_special_tokens(model, tokenizer, specials_to_add=specials_to_add)\n\n    if 'opt' in model_name:\n        tokenizer.add_bos_token=False\n    return model, tokenizer, model_config, wrapper\n\n\n\n\ndef load_plm_from_config(config: CfgNode):\n    r\"\"\"A plm loader using a global config.\n    It will load the model, tokenizer, and config simulatenously.\n\n    Args:\n        config (:obj:`CfgNode`): The global config from the CfgNode.\n\n    Returns:\n        :obj:`PreTrainedModel`: The pretrained model.\n        :obj:`tokenizer`: The pretrained tokenizer.\n        :obj:`model_config`: The config of the pretrained model.\n        :obj:`model_config`: The wrapper class of this plm.\n    \"\"\"\n    plm_config = config.plm\n    model_class = get_model_class(plm_type = plm_config.model_name)\n    model_config = model_class.config.from_pretrained(plm_config.model_path)\n    # you can change huggingface model_config here\n    # if 't5'  in plm_config.model_name: # remove dropout according to PPT~\\ref{}\n    #     model_config.dropout_rate = 0.0\n    if 'gpt' in plm_config.model_name: # add pad token for gpt\n        if \"<pad>\" not in config.plm.specials_to_add:\n            config.plm.specials_to_add.append(\"<pad>\")\n    model = model_class.model.from_pretrained(plm_config.model_path, config=model_config)\n    tokenizer = model_class.tokenizer.from_pretrained(plm_config.model_path)\n    wrapper = model_class.wrapper\n    model, tokenizer = add_special_tokens(model, tokenizer, specials_to_add=config.plm.specials_to_add)\n    return model, tokenizer, model_config, wrapper\n\ndef add_special_tokens(model: PreTrainedModel,\n                       tokenizer: PreTrainedTokenizer,\n                       specials_to_add: Optional[List[str]] = None):\n    r\"\"\"add the special_tokens to tokenizer if the special token\n    is not in the tokenizer.\n\n    Args:\n        model (:obj:`PreTrainedModel`): The pretrained model to resize embedding\n                after adding special tokens.\n        tokenizer (:obj:`PreTrainedTokenizer`): The pretrained tokenizer to add special tokens.\n        specials_to_add: (:obj:`List[str]`, optional): The special tokens to be added. Defaults to pad token.\n\n    Returns:\n        The resized model, The tokenizer with the added special tokens.\n\n    \"\"\"\n    if specials_to_add is None:\n        return model, tokenizer\n    for token in specials_to_add:\n        if \"pad\" in token.lower():\n            if tokenizer.pad_token is None:\n                tokenizer.add_special_tokens({'pad_token': token})\n                model.resize_token_embeddings(len(tokenizer))\n                logger.info(\"pad token is None, set to id {}\".format(tokenizer.pad_token_id))\n    return model, tokenizer","metadata":{"id":"71QIB8CjdRPI","execution":{"iopub.status.busy":"2022-11-25T05:36:37.453946Z","iopub.execute_input":"2022-11-25T05:36:37.454249Z","iopub.status.idle":"2022-11-25T05:36:37.484583Z","shell.execute_reply.started":"2022-11-25T05:36:37.454223Z","shell.execute_reply":"2022-11-25T05:36:37.483501Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"plm, tokenizer, model_config, WrapperClass =load_plm(\"xlm\", \"xlm-roberta-base\")","metadata":{"id":"NDPRMGSXdUwW","outputId":"c322f46e-4506-45f8-84b4-cc3804d9c171","execution":{"iopub.status.busy":"2022-11-25T05:36:37.486803Z","iopub.execute_input":"2022-11-25T05:36:37.487324Z","iopub.status.idle":"2022-11-25T05:36:42.932415Z","shell.execute_reply.started":"2022-11-25T05:36:37.487289Z","shell.execute_reply":"2022-11-25T05:36:42.929691Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87fc679295374ae7878d1c8ea6b61fd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2548d15eed447348a8fb56073618a7e"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2620609698.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWrapperClass\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mload_plm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xlm-roberta-base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_23/397028835.py\u001b[0m in \u001b[0;36mload_plm\u001b[0;34m(model_name, model_path, specials_to_add)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# model_config.resid_pdrop = 0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# model_config.embd_pdrop = 0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2013\u001b[0m                     \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2014\u001b[0m                     \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2015\u001b[0;31m                     \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2016\u001b[0m                 )\n\u001b[1;32m   2017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         )\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{url} not found in cache or force_download set to True, downloading to {temp_file.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"storing {url} in cache at {cache_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Downloading\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     )\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_bytes_read\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength_remaining\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength_remaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"from openprompt.prompts import MixedTemplate\npromptTemplate = MixedTemplate(\n    model=plm,\n    text = '{\"placeholder\": \"text_a\"} {\"soft\": \"It\"} {\"soft\": \"was\"} {\"mask\"}.',\n    tokenizer = tokenizer,\n)","metadata":{"id":"jD4_Q5w0dXqx","execution":{"iopub.status.busy":"2022-11-25T05:36:42.933232Z","iopub.status.idle":"2022-11-25T05:36:42.933649Z","shell.execute_reply.started":"2022-11-25T05:36:42.933449Z","shell.execute_reply":"2022-11-25T05:36:42.933467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = [ \n    \"0\",\n    \"1\",\n    \"2\",\n]","metadata":{"id":"DrH_VvXgdgZ4","execution":{"iopub.status.busy":"2022-11-25T05:36:42.936872Z","iopub.status.idle":"2022-11-25T05:36:42.937882Z","shell.execute_reply.started":"2022-11-25T05:36:42.937557Z","shell.execute_reply":"2022-11-25T05:36:42.937586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from openprompt.prompts import SoftVerbalizer\npromptVerbalizer = SoftVerbalizer(tokenizer, plm, num_classes=3)","metadata":{"id":"TaRA_VwEdjso","execution":{"iopub.status.busy":"2022-11-25T05:36:42.939464Z","iopub.status.idle":"2022-11-25T05:36:42.940346Z","shell.execute_reply.started":"2022-11-25T05:36:42.940064Z","shell.execute_reply":"2022-11-25T05:36:42.940091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from openprompt import PromptForClassification\npromptModel = PromptForClassification(\n    template = promptTemplate,\n    plm = plm,\n    verbalizer = promptVerbalizer,\n)","metadata":{"id":"X4RoGcShfJlz","execution":{"iopub.status.busy":"2022-11-25T05:36:42.942007Z","iopub.status.idle":"2022-11-25T05:36:42.942810Z","shell.execute_reply.started":"2022-11-25T05:36:42.942530Z","shell.execute_reply":"2022-11-25T05:36:42.942555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nMAX_LEN = 128\n\nfrom openprompt import PromptDataLoader\ndata_loader = PromptDataLoader(\n    dataset = examples,\n    tokenizer = tokenizer,\n    template = promptTemplate,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=1,\n    max_length=MAX_LEN,\n    truncation=True,\n    padding=\"max_length\"\n)","metadata":{"id":"7wS6PECof2NN","outputId":"de5284af-6084-4395-94a6-197ab149a675","execution":{"iopub.status.busy":"2022-11-25T05:36:42.944254Z","iopub.status.idle":"2022-11-25T05:36:42.945047Z","shell.execute_reply.started":"2022-11-25T05:36:42.944761Z","shell.execute_reply":"2022-11-25T05:36:42.944801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import  AdamW, get_linear_schedule_with_warmup\nloss_func = torch.nn.CrossEntropyLoss()\noptimizer_grouped_parameters = [\n    {'params': [p for n,p in promptModel.template.named_parameters() if \"raw_embedding\" not in n]}\n]\noptimizer = AdamW(optimizer_grouped_parameters, lr=1e-3)\n\noptimizer_grouped_parameters1 = [\n    {'params': promptModel.verbalizer.group_parameters_1, \"lr\":3e-5},\n    {'params': promptModel.verbalizer.group_parameters_2, \"lr\":3e-4},\n]\n\noptimizer1 = AdamW(optimizer_grouped_parameters1)\n","metadata":{"id":"_bid98egnL2r","outputId":"62c67d59-283e-48f1-db58-1bc46cc46b84","execution":{"iopub.status.busy":"2022-11-25T05:36:42.946637Z","iopub.status.idle":"2022-11-25T05:36:42.947449Z","shell.execute_reply.started":"2022-11-25T05:36:42.947186Z","shell.execute_reply":"2022-11-25T05:36:42.947212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"promptModel=promptModel.cuda()\nfor epoch in range(2):\n    tot_loss = 0\n    for step, inputs in enumerate(data_loader):\n        inputs = inputs.cuda()\n        logits = promptModel(inputs)\n        labels = inputs['label']\n        loss = loss_func(logits, labels)\n        loss.backward()\n        tot_loss += loss.item()\n        optimizer.step()\n        optimizer.zero_grad()\n        optimizer1.step()\n        optimizer1.zero_grad()\n        print(tot_loss/(step+1))","metadata":{"id":"g0jRdcVPpB8z","outputId":"5b170084-f552-423f-9b57-37ee03e2b673","execution":{"iopub.status.busy":"2022-11-25T05:36:42.949035Z","iopub.status.idle":"2022-11-25T05:36:42.949855Z","shell.execute_reply.started":"2022-11-25T05:36:42.949583Z","shell.execute_reply":"2022-11-25T05:36:42.949609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"examples_test=[]\nfrom openprompt.data_utils import InputExample\nfor i in range(len(X_test)):\n  examples_test.append(InputExample(\n        guid = i,\n        text_a =X_test[i],\n        label=y_test[i]\n    ))\n  ","metadata":{"id":"UaxDXhICnDOG","execution":{"iopub.status.busy":"2022-11-25T05:36:42.951316Z","iopub.status.idle":"2022-11-25T05:36:42.952229Z","shell.execute_reply.started":"2022-11-25T05:36:42.951914Z","shell.execute_reply":"2022-11-25T05:36:42.951954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_loader = PromptDataLoader(\n    dataset = examples_test,\n    tokenizer = tokenizer,\n    template = promptTemplate,\n    tokenizer_wrapper_class=WrapperClass,\n    batch_size=1,\n    max_length=MAX_LEN,\n    truncation=True,\n    padding=\"max_length\"\n)","metadata":{"id":"eQIWD-aHgfDQ","outputId":"2aae1828-ab3c-4d7a-ba7e-bfd1eee91ad7","execution":{"iopub.status.busy":"2022-11-25T05:36:42.953891Z","iopub.status.idle":"2022-11-25T05:36:42.954809Z","shell.execute_reply.started":"2022-11-25T05:36:42.954497Z","shell.execute_reply":"2022-11-25T05:36:42.954525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\nallpreds = []\nalllabels = []\npromptModel=promptModel.cuda()\nfor step, inputs in enumerate(data_loader):\n    inputs=inputs.cuda();\n    logits = promptModel(inputs)    \n    labels = inputs['label']\n    alllabels.extend(labels.cpu().tolist())\n    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n","metadata":{"id":"_i1RAT7DgDyV","execution":{"iopub.status.busy":"2022-11-25T05:36:42.956294Z","iopub.status.idle":"2022-11-25T05:36:42.957101Z","shell.execute_reply.started":"2022-11-25T05:36:42.956833Z","shell.execute_reply":"2022-11-25T05:36:42.956858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef compute_metrics(allpreds,alllabels):\n    metric1 = load_metric(\"precision\")\n    metric2 = load_metric(\"recall\")\n    metric3 = load_metric(\"f1\")\n    metric4 = load_metric(\"accuracy\")\n    \n    predictions, labels = allpreds,alllabels\n    precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n    recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n    f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    macro_precision = metric1.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n    macro_recall = metric2.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n    macro_f1 = metric3.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n    return {\"accuracy\":accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"macro_precision\": macro_precision, \"macro_recall\": macro_recall, \"macro_f1\": macro_f1}","metadata":{"id":"Csmm3ou8sKAp","execution":{"iopub.status.busy":"2022-11-25T05:36:42.959138Z","iopub.status.idle":"2022-11-25T05:36:42.959969Z","shell.execute_reply.started":"2022-11-25T05:36:42.959680Z","shell.execute_reply":"2022-11-25T05:36:42.959707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compute_metrics(allpreds,alllabels)","metadata":{"id":"RA4T07_wowtR","outputId":"376d90c8-054d-4aed-b126-d28c50009156","execution":{"iopub.status.busy":"2022-11-25T05:36:42.961554Z","iopub.status.idle":"2022-11-25T05:36:42.962408Z","shell.execute_reply.started":"2022-11-25T05:36:42.962144Z","shell.execute_reply":"2022-11-25T05:36:42.962170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c=0\nfor i,j in zip(allpreds, alllabels):\n  if(i!=j):\n    print(c,X_test[c],\"---->\",\"predicted :\", uniq[i],\" actual :\",  uniq[j])\n\n  c+=1","metadata":{"id":"BMfB84xM0b3h","outputId":"899612f0-2384-4e38-afa1-aee0384514d0","execution":{"iopub.status.busy":"2022-11-25T05:36:42.963895Z","iopub.status.idle":"2022-11-25T05:36:42.964688Z","shell.execute_reply.started":"2022-11-25T05:36:42.964428Z","shell.execute_reply":"2022-11-25T05:36:42.964454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c=0\nfor i,j in zip(allpreds, alllabels):\n  if(i==j):\n    print(c,X_test[c],\"---->\",\"predicted :\", uniq[i],\" actual :\",  uniq[j])\n    \n  c+=1","metadata":{"id":"d-aRTvUG9jR7","outputId":"893be599-87cd-45f8-b433-787791d6a7bf","execution":{"iopub.status.busy":"2022-11-25T05:36:42.966131Z","iopub.status.idle":"2022-11-25T05:36:42.966982Z","shell.execute_reply.started":"2022-11-25T05:36:42.966654Z","shell.execute_reply":"2022-11-25T05:36:42.966680Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"zs7_5iaFDRaO"},"execution_count":null,"outputs":[]}]}